---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Name's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: "Roboto Flex"
    monofont: InputMonoCondensed
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, JueLin Liu, Yingxi Zhou, Yale Lu, Yuanqing Zhang, Kun Li , pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date:17/12/2024

Student Numbers:24073575 JueLin Liu, 24036199 Yingxi Zhou, 23225727 Yale Lu, 24068573 Yuanqing Zhang, 23157049 Kun Li

## Brief Group Reflection

|    What Went Well     | What Was Challenging |
|    --------------     | -------------------- |
|   Regression Equation | Data Collection      |


## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?



{{< pagebreak >}}

# Response to Questions

See the raw file for examples of how to hide computational output as there is code hidden here.

```{bash}
# Install the required Python packages
pip install numpy pandas matplotlib seaborn scipy scikit-learn statsmodels geopandas shapely pyproj libpysal esda mgwr splot
```

```{python, include=FALSE}
#| echo: False
import pandas as pd
import geopandas as gpd
from libpysal.weights import Queen
from esda import Moran, Moran_Local
import matplotlib.pyplot as plt
from mgwr.gwr import GWR
import pyproj
import numpy as np
from sklearn.preprocessing import StandardScaler

# Read Airbnb “Entire home/apt” data
data_folder = "https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/refs/heads/main/data"  # URL base for data files
shapefile_folder = "https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/refs/heads/main/data/ESRI"  # URL base for shapefiles

Enprice_file = f"{data_folder}/Enprice.csv"
shapefile = f"{shapefile_folder}/London_Borough_Excluding_MHW.zip"

price_by_neighbourhood = pd.read_csv(Enprice_file)
geo_data = gpd.read_file(shapefile)
geo_data = geo_data.merge(price_by_neighbourhood, left_on="NAME", right_on="neighbourhood")
geo_data['price'] = pd.to_numeric(geo_data['price'], errors='coerce')

# Draw “Entire home/apt” price map
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
geo_data.plot(column='price',
              cmap='coolwarm',
              legend=True,
              linewidth=0.5,
              edgecolor='black',
              ax=ax)

ax.set_title("Airbnb “Entire home/apt” price - borough", fontsize=15)
ax.set_axis_off()
plt.show()

# price Moran's I test
weights = Queen.from_dataframe(geo_data)
weights.transform = "R"

moran = Moran(geo_data["price"], weights)
print(f"Global Moran's I: {moran.I}, p-value: {moran.p_sim}")

# Read other data（income median, population density，airbnb Entire home density）,merge all data
income_pp_file = f"{data_folder}/social-eco data.xlsx"
income_pp_df = pd.read_excel(income_pp_file)

mean_price_file = f"{data_folder}/merged_rental_and_density.csv"
mean_price_df = pd.read_csv(mean_price_file)

merged_data = geo_data.merge(income_pp_df, left_on="NAME", right_on="NAME", how="left")
merged_data = merged_data.merge(mean_price_df, left_on="NAME", right_on="NAME", how="left")

# Extract the latitude and longitude of the center point
merged_data['centroid'] = merged_data.geometry.centroid
merged_data['latitude'] = merged_data['centroid'].y
merged_data['longitude'] = merged_data['centroid'].x

# Set projection: OSGB36 (UK National Grid) to WGS84 (Latitude and longitude coordinate system)
projection = pyproj.Transformer.from_crs("EPSG:27700", "EPSG:4326", always_xy=True)

def convert_to_lat_lon(x, y):
    lon, lat = projection.transform(x, y)
    return lat, lon

merged_data[['latitude', 'longitude']] = merged_data.apply(
    lambda row: pd.Series(convert_to_lat_lon(row['longitude'], row['latitude'])),
    axis=1
)


# Independent and dependent variables of GWR were extracted and standardized
y = merged_data["borough_weighted_average"].values.reshape((-1, 1))
X = merged_data[["price", "Density", "income-median", "Population per hectare"]].values
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_std = scaler_X.fit_transform(X)
y_std = scaler_y.fit_transform(y)

# Execute the GWR model and view the regression results
coords = [(lat, lon) for lat, lon in zip(merged_data['latitude'], merged_data['longitude'])]
bw = 20
gwr_model = GWR(coords, y_std, X_std, bw)
gwr_results = gwr_model.fit()
gwr_model = GWR(coords, y, X, bw)
gwr_results = gwr_model.fit()
print("GWR results:", gwr_results.summary())

# Get the GWR coefficient of Airbnb “Entire home/apt” price and visualize it
gwr_coefficients = gwr_results.params[:, 1]
gwr_coefficients_abs = np.abs(gwr_coefficients)

merged_data['gwr_coefficients_abs'] = gwr_coefficients
fig, ax = plt.subplots(figsize=(12, 10))
from matplotlib.colors import LinearSegmentedColormap
import matplotlib.pyplot as plt
red_cmap = LinearSegmentedColormap.from_list("custom_red", ["#F5F8FA", "#990000"])
merged_data.plot(column='gwr_coefficients_abs',
                 ax=ax,
                 legend=True,
                 legend_kwds={'label': "Airbnb “Entire home/apt” price-GWR Coefficients",
                              'orientation': "horizontal"},
                 cmap=red_cmap,
                 edgecolor='k', linewidth=0.5)
plt.title("Airbnb “Entire home/apt” price-GWR Coefficients Visualization", fontsize=16)
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

# Get the GWR coefficient of Airbnb “Entire home/apt” Density and visualize it
gwr_coefficients = gwr_results.params[:, 2]
gwr_coefficients_abs = np.abs(gwr_coefficients)
merged_data['gwr_coefficients_abs'] = gwr_coefficients
fig, ax = plt.subplots(figsize=(12, 10))
blue_cmap = LinearSegmentedColormap.from_list("custom_blue", ["#F5F8FA", "#133F7F"])
merged_data.plot(column='gwr_coefficients_abs',
                 ax=ax,
                 legend=True,
                 legend_kwds={'label': "Airbnb “Entire home/apt” Density-GWR Coefficients",
                              'orientation': "horizontal"},
                 cmap=blue_cmap,
                 edgecolor='k', linewidth=0.5)

plt.title("Airbnb “Entire home/apt” Density-GWR Coefficients Visualization", fontsize=16)
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()
```

```{python, include=FALSE}
import pandas as pd
import statsmodels.api as sm

# Regression analysis

file_url = "https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/refs/heads/main/data/borough_combined_data_2023.csv"

regression_data = pd.read_csv(file_url)

print(regression_data.head()) 
print(regression_data.dtypes)

# Check for NaNs in the specified columns
nan_check = regression_data[['Density', 'weighted_price', 'income-median', 
                             'Population per hectare', 'unemploy_rate']].isna().sum()
print("Missing Values (NaNs):\n", nan_check)

import numpy as np

# Check for positive and negative infinite values
inf_check_pos = (regression_data[['Density', 'weighted_price', 'income-median', 
                                  'Population per hectare', 'unemploy_rate']] == np.inf).sum()
inf_check_neg = (regression_data[['Density', 'weighted_price', 'income-median', 
                                  'Population per hectare', 'unemploy_rate']] == -np.inf).sum()

print("Positive Infinite Values:\n", inf_check_pos)
print("Negative Infinite Values:\n", inf_check_neg)

# Define dependent and independent variables

y = regression_data['mean_all']
X =  regression_data[['Density', 'weighted_price', 'meadian_earnings','ln_population','unemploy_rate']]

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Calculate VIF
X_no_const = X
X_with_const = sm.add_constant(X_no_const)

# Output VIF value
vif = pd.DataFrame()
vif['Variable'] = X_with_const.columns
vif['VIF'] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]

print(vif)

X = sm.add_constant(X)

#Build a regression model
model = sm.OLS(y, X).fit()

# Output regression results
print(model.summary())

# Define grouped regression function
# Here type=0 is Inner London, type=1 is Outer London
def run_regression(data, group_value):
    group_data = data[data['type'] == group_value]
    
   # Define dependent and independent variables
    y = group_data['Median_One Bedroom']
    y = group_data['mean_all']
    X = group_data[['Density', 'weighted_price', 'meadian_earnings', 'ln_population', 'unemploy_rate']]
    X = sm.add_constant(X)
    
    # Regression analysis
    model = sm.OLS(y, X).fit()
    print(f"Regression results: type = {group_value}\n")
    print(model.summary())
    return model

# Grouped regression
model_type_0 = run_regression(regression_data, 0)
model_type_1 = run_regression(regression_data, 1)


## Considering that there are significant spatial differences between inner and outer London (see Moran I clustering results), the London boroughs are further divided into inner and outer London for regression analysis. Table m shows the regression results of inner and outer London. 

## It can be seen that the impact of airbnb density in inner London on the rental price of the borough is still significant, and the coefficient is also larger than the benchmark regression. The impact of outer London is not significant. This may be because inner London is home to many tourist attractions and cultural heritages, and is the core of the tourism and short-term rental market. At the same time, Airbnb has increased the demand for housing in the rental market, resulting in a decrease in the supply of long-term rentals, which has driven up rents. Local residents may convert long-term rentals to short-term Airbnb rentals, reducing the number of available rentals on the market. Outer London is dominated by residential and commuter areas, and the rental market is less correlated with the short-term tourism rental market. The housing supply is more sufficient than that in Inner London, the balance between supply and demand is better, and the Airbnb activity is lower than that in Inner London.

## In order to determine whether Airbnb density has a lag effect and an error effect in space, we regress Airbnb density using a spatial lag model and a spatial error model respectively. The results are shown in Table K. The coefficients of the spatial lag term and the spatial error term are not significant, indicating that the above effects do not exist in space.

from pysal.model import spreg
from libpysal.weights import Queen
import geopandas as gpd
import pandas as pd

# Read shp file
url = "https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/main/data/borough_merged.zip"

gdf_merged = gpd.read_file(url)

# Check data columns
print(gdf_merged.columns)

# Create a spatial weight matrix based on the queen adjacency rule
w = Queen.from_dataframe(gdf_merged)
w.transform = 'R' 

print(gdf_merged.columns)

# Define dependent and independent variables
y = gdf_merged['mean_all'].values.reshape(-1, 1)
X = gdf_merged[['Density']].values

model_sar = spreg.ML_Lag(
    y, X, w, 
    name_y='mean_all', 
    name_x=['Density']
)

print(model_sar.summary)

# Run the spatial error model
model_sem = spreg.ML_Error(y, X, w, name_y='mean_all', 
                           name_x=['Density'])

# Print model results
print(model_sem.summary)
```

```{python, include=FALSE}
#| echo: False
import geopandas as gpd
import pandas as pd
from shapely.geometry import Point
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from matplotlib.colors import ListedColormap
from libpysal.weights import KNN  # Import KNN
from esda.moran import Moran, Moran_Local
from splot.esda import moran_scatterplot, lisa_cluster
from sklearn.metrics import silhouette_score
from scipy.stats import pearsonr

#  read in data
file_url = "https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/main/data/cleaned_listings.csv"
df = pd.read_csv(file_url)

#  creat the points
geometry = [Point(xy) for xy in zip(df["longitude"], df["latitude"])]

#  Creat a GeoDataFrame，define the original CRS as WGS84
gdf = gpd.GeoDataFrame(df, geometry=geometry, crs="EPSG:4326")

# projection to the British CRS
gdf_projected = gdf.to_crs("EPSG:27700")

# read LOSA
file_url = "https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/main/data/LSOA_2011_London_gen_MHW.zip"
lsoa = gpd.read_file(file_url)

# Visualization
lsoa.plot(figsize=(10, 10), edgecolor="black")

# Set the title and display
plt.title("City Locations")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()

points_gdf = gdf.to_crs(lsoa.crs)
# spatial join
points_with_regions = gpd.sjoin(points_gdf, lsoa, how="left", predicate="within")

# add columns
points_with_regions = points_with_regions.rename(columns={"LSOA11CD": "LSOA11CD"})

# show the result
print(points_with_regions[["id", "longitude", "latitude", "LSOA11CD"]])

# visualization
fig, ax = plt.subplots(figsize=(10, 10))

# build the boundary
lsoa.plot(ax=ax, color="lightgrey", edgecolor="black")

# plot the points
points_with_regions.plot(ax=ax, color="red", markersize=2)
plt.title("Points and Regions")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()

# calculate the ares
lsoa["Area_m2"] = lsoa.geometry.area
lsoa["Area_km2"] = lsoa["Area_m2"] / 1_000_000

# show the result
print(lsoa[["LSOA11CD","Area_km2"]].head())

# Ensure consistent coordinate systems
if gdf.crs != "EPSG:27700":
    gdf = gdf.to_crs("EPSG:27700")

if lsoa.crs != "EPSG:27700":
    lsoa = lsoa.to_crs("EPSG:27700")

# spatial join
points_in_lsoa = gpd.sjoin(gdf, lsoa, how="left", predicate="within")

# 4. Count the points by area (LSOA11CD)
point_counts = points_in_lsoa.groupby("LSOA11CD").size().reset_index(name="Point_Count")
lsoa = lsoa.merge(point_counts, on="LSOA11CD", how="left")


# Fill NaN values with 0
lsoa["Point_Count"] = lsoa["Point_Count"].fillna(0)
lsoa["Density"] = lsoa["Point_Count"] / lsoa["Area_km2"]

# Extract the geometric center and density values of the area.
lsoa["centroid"] = lsoa.geometry.centroid  
lsoa["x"] = lsoa["centroid"].x  
lsoa["y"] = lsoa["centroid"].y 
cluster_data = lsoa[["x", "y", "Density"]].values

# Data normalization
scaler = StandardScaler()
cluster_data_scaled = scaler.fit_transform(cluster_data)

# Get the suitable number of clusterings by silhouette_score

silhouette_scores = []
for k in range(2, 10):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(cluster_data_scaled)
    score = silhouette_score(cluster_data_scaled, labels)
    silhouette_scores.append(score)

plt.plot(range(2, 10), silhouette_scores, marker='o')
plt.title("Silhouette Score for K-Means LSOA Clustering")
plt.xlabel("Number of Clusters")
plt.ylabel("Silhouette Score")
plt.show()

# Make clusterings of 6
n_clusters = 6 
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
lsoa["Cluster1"] = kmeans.fit_predict(cluster_data_scaled)  

# Visualization using seaborn color palette
fig, ax = plt.subplots(1, 1, figsize=(10, 8))

# Choose a seaborn color palette and convert it to a ListedColormap
palette = sns.color_palette("coolwarm", n_colors=n_clusters)  # generate viridis color palette
cmap = ListedColormap(palette.as_hex())  # Convert palette to ListedColormap

# Plot using the chosen color palette with reduced transparency (alpha=1)
lsoa.plot(column="Cluster1", cmap=cmap, legend=True, ax=ax, alpha=1,linewidth=0.1)

plt.title("K-Means Clustering of LSOA by Spatial and Density Features")
plt.savefig("kmeans_clustering_lsoadensity_viridis.png", dpi=300, bbox_inches="tight")
plt.show()

# Create KNN contiguity weight matrix with k=5 (5 nearest neighbors).
k = 5  # Number of nearest neighbors
w = KNN.from_dataframe(lsoa, k=k)  # Create KNN weight matrix
# Weight matrix normalization.
w.transform = "r"

# Calculate global Moran's I.
moran = Moran(lsoa["Density"], w)
print(f"Moran's I: {moran.I}")
print(f"p-value: {moran.p_sim}")

# Calculate local Moran's I
lisa = Moran_Local(lsoa["Density"], w)

# Add cluster types and significance to the lsoa dataframe
lsoa["lisa_cluster"] = lisa.q  
lsoa["lisa_sig"] = lisa.p_sim  

# Cluster type description:
# 1 = High-High: High-value areas surrounded by high-value neighbors (High-value clusters)
# 2 = Low-Low: Low-value areas surrounded by low-value neighbors (Low-value clusters)
# 3 = High-Low: High-value areas surrounded by low-value neighbors (High-value outliers)
# 4 = Low-High: Low-value areas surrounded by high-value neighbors (Low-value outliers)

# Plot the LISA results
fig, ax = plt.subplots(1, 1, figsize=(12, 8))
lisa_cluster(lisa, lsoa, p=0.05, ax=ax)
plt.title("LISA Cluster Map by LSOA (KNN Moran's I)")
plt.savefig("12_Moran'I_lsoadensity_knn.png", dpi=300, bbox_inches="tight")
plt.show()

# Extract the required columns.
Moran_I = bor[["GSS_CODE","NAME", "lisa_cluster","lisa_sig"]]

# save as csv
Moran_I.to_csv("boroughdensity_Moran_I.csv", index=False, encoding="utf-8")

# read listings
listings = pd.read_csv("data/listings.csv", encoding="ISO-8859-1")

# Keep the required columns.
columns_to_keep = ['calculated_host_listings_count','id', 'latitude', 'longitude', 'price', 'host_id']
listings = listings[columns_to_keep]

# Remove missing values.
listings = listings.dropna()

# save as a file
listings.to_csv("processed_listings.csv", index=False)

#  creat the points
geometry = [Point(xy) for xy in zip(listings["longitude"], df["latitude"])]

#  Creat a GeoDataFrame，define the original CRS as WGS84
gdf2 = gpd.GeoDataFrame(listings, geometry=geometry, crs="EPSG:4326")

# projection to the British CRS
gdf2_projected = gdf2.to_crs("EPSG:27700")
points_gdf2 = gdf2.to_crs(bor.crs)

# Spatial Join: Linking point data with area data.
points_with_regions2 = gpd.sjoin(points_gdf2, bor, how="left", predicate="within")

# Add the column as names of boroughs
points_with_regions2 = points_with_regions2.rename(columns={"GSS_CODE": "GSS_CODE"})

# make sure the same CRS
if gdf2.crs != bor.crs:
    gdf2 = gdf2.to_crs(bor.crs)
# use Seaborn color
sns_colors = sns.color_palette("deep") 
blue = sns_colors[0]  
red = sns_colors[3]   

# plot
fig, ax = plt.subplots(figsize=(10, 10))

# background is bor
bor.plot(ax=ax, color="lightgrey", edgecolor="black", linewidth=0.5, alpha=0.7)

# make the point plot
gdf2[gdf2['calculated_host_listings_count'] == 1].plot(
    ax=ax, color=blue, label="Single-unit Host", markersize=4
)
gdf2[gdf2['calculated_host_listings_count'] > 1].plot(
    ax=ax, color=red, label="Multi-unit Host", markersize=4
)

# Add a title and legend.
ax.set_title("Single-unit vs Multi-unit Hosts", fontsize=14)
ax.legend()

# show the result
plt.savefig("Single-unit vs Multi-unit Hosts.png", dpi=300, bbox_inches="tight")
plt.show()

# Group by GSS_CODE and calculate the count of id and the number of unique host_id for each area.
result = points_with_regions2.groupby("GSS_CODE").agg(
    id_count=("id", "count"),                
    host_id_count=("host_id", "nunique")     
).reset_index()

# Calculate the average number of listings per host.
result["id_host_ratio"] = result["id_count"] / result["host_id_count"]
points_with_regions2 = points_with_regions2.merge(
    result[["GSS_CODE", "id_count", "host_id_count", "id_host_ratio"]],
    on="GSS_CODE",
    how="left"
)

# read final_neighbourhood_prices.csv 
file_url = "https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/main/data/final_neighbourhood_prices.csv"
neighbourhood_prices = pd.read_csv(file_url)

print(neighbourhood_prices.head())
print(points_with_regions2.head())

# join 'weighted_price'to points_with_regions2 
points_with_prices = points_with_regions2.merge(
    neighbourhood_prices[['neighbourhood', 'weighted_price']],  
    left_on='NAME',    
    right_on='neighbourhood',  
    how='left'        
)
points_data = points_with_prices[['NAME', 'weighted_price', 'id_host_ratio']]
# join with bor and new data
bor = bor.merge(
    points_data.groupby('NAME', as_index=False).mean(), 
    on='NAME',      
    how='left'      
)

# Extract the weighted_price and id_host_ratio columns and remove missing values.
data = bor[['weighted_price', 'id_host_ratio']].dropna()

#  Calculate the Pearson correlation coefficient and p-value.
pearson_coef, p_value = pearsonr(data['weighted_price'], data['id_host_ratio'])
print("\nPearson Correlation Coefficient:", pearson_coef)
print("P-value:", p_value)
```

```{python, include=FALSE}
#| echo: False
# density_analysis
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt

# Read density data
density_file = "https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/main/data/density_data.csv"
shapefile_zip = "https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/main/data/London_Borough_Excluding_MHW.zip"

density_df = pd.read_csv(density_file)
shape_gdf = gpd.read_file(shapefile_zip)

# Merge shapefile with density data
merged_gdf = shape_gdf.merge(density_df, left_on="NAME", right_on="borough")

# Plot density map
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
merged_gdf.plot(column="density", cmap="OrRd", legend=True, ax=ax)
plt.title("Airbnb Density by Borough")
plt.savefig("./output_data/density_map.png")
plt.show()


# entireroom_density
# Analysis of entire room density
entire_home_file = "https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/main/data/entire_home_density.csv"
entire_home_df = pd.read_csv(entire_home_file)

# Load the data
entire_home_df = pd.read_csv(entire_home_file)
shape_gdf = gpd.read_file(shapefile)

# Merge with shapefile
merged_entire_gdf = shape_gdf.merge(entire_home_df, left_on="NAME", right_on="borough")

# Plot map
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
merged_entire_gdf.plot(column="entire_home_density", cmap="Blues", legend=True, ax=ax)
plt.title("Entire Home Density by Borough")
plt.savefig("./output_data/entire_home_density_map.png")
plt.show()


# weighted_average_andmerge
# Weighted Average Calculation and Merge
weighted_avg_file = "https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/main/data/weighted_average.csv"
additional_data_file = "https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/main/data/additional_data.csv"
weighted_avg_df = pd.read_csv(weighted_avg_file)
additional_df = pd.read_csv(additional_data_file)

# Load files
weighted_avg_df = pd.read_csv(weighted_avg_file)
additional_df = pd.read_csv(additional_data_file)

# Merge data
merged_df = pd.merge(weighted_avg_df, additional_df, on="borough")

# Calculate weighted average price
merged_df["weighted_avg_price"] = merged_df["price"] * merged_df["weight"]

# Save results
output_path = "./output_data/weighted_avg_results.csv"
merged_df.to_csv(output_path, index=False)

print("Weighted average calculation completed. Results saved to:", output_path)
```

```{python, include=FALSE}
import pandas as pd
import matplotlib.pyplot as plt
import os

# Define input and output paths
input_path = "https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/main/data/combined_bedroom_category_long.csv"
output_folder = './output'
os.makedirs(output_folder, exist_ok=True)

# Load the data
df = pd.read_csv(input_path)

# Ensure Count_of_Rents is numeric
df['Count_of_Rents'] = pd.to_numeric(df['Count_of_Rents'], errors='coerce')
df = df.dropna(subset=['Count_of_Rents'])  # Remove rows with invalid values
df['Count_of_Rents'] = df['Count_of_Rents'].astype(int)

# --- Data Analysis ---

# 1. Total number of Airbnb listings over years
total_listings = df.groupby('Year')['Count_of_Rents'].sum().reset_index()
total_listings.columns = ['Year', 'Total_Rents']

# 2. Adjust room type: keep 'Room', aggregate others as 'Entire home'
df['Bedroom_Category_Adjusted'] = df['Bedroom_Category'].apply(
    lambda x: 'Room' if 'Room' in x else 'Entire home'
)

# Group data for adjusted room types
room_type_summary = df.groupby(['Year', 'Bedroom_Category_Adjusted'])['Count_of_Rents'].sum().reset_index()
room_type_summary.columns = ['Year', 'Bedroom_Category', 'Total_Rents']

# 3. Top boroughs with the most listings and their room type distribution
top_boroughs = df.groupby('Borough')['Count_of_Rents'].sum().nlargest(3).index  # Top 3 boroughs
top_regions_summary = df[df['Borough'].isin(top_boroughs)].groupby(
    ['Year', 'Borough', 'Bedroom_Category_Adjusted']
)['Count_of_Rents'].sum().reset_index()
top_regions_summary.columns = ['Year', 'Borough', 'Bedroom_Category', 'Total_Rents']

# Get a sorted list of years
years = sorted(df['Year'].unique())

# --- Data Visualization ---

# Visualization 1: Total Airbnb listings over years
plt.figure(figsize=(10, 6))
plt.plot(total_listings['Year'], total_listings['Total_Rents'], marker='o', linestyle='-', color='b')
plt.title("Total Airbnb Listings Change Over Years")
plt.xlabel("Year")
plt.ylabel("Total Listings")
plt.xticks(years, rotation=45)  # Ensure x-axis has integer years
plt.grid(True)
plt.savefig(os.path.join(output_folder, "total_listings_trend.png"))
plt.show()

# Visualization 2: Change in adjusted room types over years
plt.figure(figsize=(12, 8))
for room_type in room_type_summary['Bedroom_Category'].unique():
    subset = room_type_summary[room_type_summary['Bedroom_Category'] == room_type]
    plt.plot(subset['Year'], subset['Total_Rents'], marker='o', linestyle='-', label=room_type)

plt.title("Change in Room Types Over Years")
plt.xlabel("Year")
plt.ylabel("Number of Listings")
plt.xticks(years, rotation=45)
plt.legend(title="Room Type")
plt.grid(True)
plt.savefig(os.path.join(output_folder, "adjusted_room_type_trend.png"))
plt.show()

# Visualization 3: Top boroughs and room type distribution
plt.figure(figsize=(12, 8))
for borough in top_boroughs:
    subset = top_regions_summary[top_regions_summary['Borough'] == borough]
    for category in subset['Bedroom_Category'].unique():
        subdata = subset[subset['Bedroom_Category'] == category]
        plt.bar(subdata['Year'] + (0.1 * top_boroughs.tolist().index(borough)),
                subdata['Total_Rents'], label=f"{borough} - {category}")

plt.title("Top Boroughs and Their Room Type Distribution")
plt.xlabel("Year")
plt.ylabel("Total Listings")
plt.xticks(years, rotation=45)
plt.legend(title="Borough and Room Type", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.savefig(os.path.join(output_folder, "top_boroughs_room_type.png"))
plt.show()

# --- Save Results ---
total_listings.to_csv(os.path.join(output_folder, "total_listings_summary.csv"), index=False)
room_type_summary.to_csv(os.path.join(output_folder, "adjusted_room_type_summary.csv"), index=False)
top_regions_summary.to_csv(os.path.join(output_folder, "top_regions_summary.csv"), index=False)

print("Analysis completed. Results and visualizations have been saved successfully.")
```

## 1. Who collected the InsideAirbnb data?

::: {.duedate}

( 2 points; Answer due Week 7 )

Murray Cox is the founder and current chief data activist of Inside Airbnb. He conceived the project, compiled and analyzed the data, and built the site. A variety of collaborators and partners interested in InsideAirbnb also contributed to data updates and maintenance, for example Taylor Higgins, Michael "Ziggy" Mintz, and so on.
:::

An inline citation example: As discussed on @insideairbnb, there are many...

A parenthetical citation example: There are many ways to research Airbnb [see, for example, @insideairbnb]... 

## 2. Why did they collect the InsideAirbnb data?

::: {.duedate}

( 4 points; Answer due Week 7 )

Because they aim to provide data and advocacy about Airbnb's impact on residential communities. And they work towards a vision where communities are empowered with data and information to understand, decide and control the role of renting residential homes to tourists.

:::

```{python}
#| output: asis
print(f"One of way to embed output in the text looks like this: after cleaning, we were left with {df.shape[0]:,} rows of data.")
```

This way is also supposed to work (`{python} f"{df.shape[0]:,}" `) but I've found it less reliable.

```{python}
ax = df.host_listings_count.plot.hist(bins=50);
ax.set_xlim([0,500]);
```

## 3. How did they collect it?

::: {.duedate}

( 5 points; Answer due Week 8 )
Inside Airbnb collects its data by scraping publicly available information from Airbnb's website. This process involves extracting details such as listing descriptions, host information, availability calendars, and user reviews. The collected data is then verified, cleansed, analyzed and aggregated.
:::

## 4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?

::: {.duedate}

( 11 points; Answer due Week 9 )

:::

InsideAirbnb collects data by scraping publicly available Airbnb listings, a method that can introduce certain limitations. For instance, inactive or off-platform listings may be missing, leading to incomplete coverage. Scraping errors and Airbnb’s deliberate data obfuscation, hiding exact locations or limiting transparency about occupancy, can also affect data accuracy. Additionally, because listings can be deactivated, modified, or newly added between scraping sessions, the dataset may become outdated over time.

While the data provides a useful snapshot of Airbnb’s activity, it does not fully capture the complexities of host and guest interactions. This raises concerns about representation bias, as hidden transactions remain unaccounted for. Privacy issues also emerge from using personal data without explicit consent. Furthermore, relying on incomplete data could result in misleading conclusions for urban planning and policy-making.


## 5. What ethical considerations does the use of the InsideAirbnb data raise? 

::: {.duedate}

( 18 points; Answer due {{< var assess.group-date >}} )
First, informed consent and privacy issues. Inside Airbnb is obtained by scraping data from the Airbnb website. The possible problem is that hosts and Airbnb users do not know that their data is being collected, which raises transparency and autonomy issues. At the same time, the scraped data includes property addresses, personal comments, etc., which may cause harassment or privacy violations.
Second, representative bias. Airbnb listings may be concentrated in administrative areas with better geographical (such as inner London) and socioeconomic conditions, while low-income communities and underrepresented communities with fewer Airbnb listings may be ignored or marginalized. At the same time, hosts on Airbnb do not represent the general population. They usually tend to be higher-income people and do not include workers living in public housing. Conclusions based solely on Airbnb data may ignore diverse housing needs. In addition, Airbnb's search algorithm may give priority to highly rated, high-priced, and more active listings, ignoring inactive or low-rent listings, resulting in the scraped data reflecting platform bias and exaggerating market activities.

:::

## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London? 

::: {.duedate}
### 1.Trends in London's Airbnb Market (2019-2023) 
From 2019 to 2023, London’s Airbnb market peaked in 2021 after a pandemic-related dip, then stabilised around 44,000-46,000 listings. Entire homes dominated throughout, reflecting strong, consistent demand, while room listings remained minimal. Croydon and Kensington and Chelsea led in entire home listings, indicating stable, high-demand rental markets. Overall, the market has recovered and stabilised, driven by entire home rentals.

### 2.Geographical Distribution of Airbnb Listings 
First, after analyzing Airbnb listing patterns in London using K-means clustering and Moran’s I revealed a clear spatial trend: central London has densely clustered listings, while outer areas have fewer. Similar results were confirmed at the Borough level, where rental price data was available.

![fig4](https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/refs/heads/main/fig/fig4.jpg)

The kernel density analysis result show that Airbnb listings in London cluster in central areas like Westminster, the West End, and King’s Cross due to tourist attractions and good transport links. Entire homes dominate the city center, offering privacy at higher prices, while private rooms are more dispersed in residential areas like Hackney and Islington, catering to budget travelers. Shared rooms are rare and mostly found in outer neighborhoods, reflecting lower demand.

![fig2(a)](https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/refs/heads/main/fig/fig2(a).png)

### 3.Pricing Differences Across London's Airbnb Listings
According to the statistics of Airbnb listing data in 2023, the majority of Airbnb listings are "Entire home/apt", accounting for 66.8%. In addition, from the average rent distribution diagram of "Entire home/apt" housing type in Figure 1, it can be seen that there are significant differences in rent in different administrative regions. City centres such as Westminster and Kensington&Chelsea rent for up to £350 a night, while outside the city centre rates are relatively low.

![fig3](https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/refs/heads/main/fig/fig(3).png)

:::

## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London? 

::: {.duedate}
### 1.Impact of Airbnb Density on Rental Prices in London
First, we tested the relationship between Airbnb density and rental prices in London using a regression model. The results showed that Airbnb density positively affects rental prices, raising them by about £1.25 per listing, with no multicollinearity issues (VIF < 5). Dividing London into inner and outer boroughs revealed that Airbnb density significantly impacts rental prices in inner London but not in outer areas, indicating spatial differences. However, further tests using spatial lag and spatial error models found no significant spatial effects.

### 2.Spatial Variations in Airbnb's （entire home/apt）Impact on Rent Prices
To assess the spatial impact of Airbnb's "Entire home/apt" listing density and price on monthly rent levels in different London boroughs. This report uses a geographically weighted regression (GWR) model, a spatial statistical method that takes into account the effects of geographic location on the regression coefficients to provide a more accurate estimate of regional differences. Compared with traditional global regression models, GWR models can reveal spatial differences in the relationship between dependent and independent variables in different regions, thus providing region-specific insights for policy makers. In order to fully understand the market dynamics, the model also includes population density and mean income as control variables to reveal the determinants of rent levels in different regions.
As shown in Figures below，we found that the impact of Airbnb's "Entire home/apt" type of housing density and price on rents is spatially significant, reflecting differences in economic structure, housing demand and market competition in different parts of London. Specifically, the regression coefficients of Airbnb listings density and housing prices show significant differences in space. For example, the regression coefficient of Airbnb listings density is higher in some regions, such as the downtown and northwest, showing a strong positive effect, while the effect is smaller or even negative in other regions, which may be related to the saturation of housing supply in the region or the lack of demand in the rental market. On the other hand, Airbnb prices have a greater impact on rental levels and a higher regression coefficient in the downtown and southern regions, while the effect is gradually weakened in the northern region, which may reflect the intense market competition and high short-term rental price sensitivity in the southern region.

![fig4](https://raw.githubusercontent.com/Pink-Fur-BuBBles/Group-Work-for-FSDS/refs/heads/main/fig/fig4.jpg)

### 3.Conclusion: Airbnb's Influence on London's Housing Market
The regression analysis shows that more Airbnb listings in inner London push up local rental prices by reducing long-term housing supply. This happens because property owners switch to short-term rentals for higher profits, attracting tourists and driving up demand. As a result, local residents face rising rents and housing shortages. According to Garay, Morales, and Wilson (2020), long-term rentals are mainly needed by local residents, while short-term rentals are mostly used by tourists, making local properties more valuable and expensive. That’s why many communities resist Airbnb.To ease this pressure, policymakers could limit short-term rentals, enforce host registration and taxes, and promote affordable long-term housing development. 
Based on the results of GWR analysis, the government can adopt regionally differentiated Airbnb regulatory policies. In areas where the density of Airbnb “Entire home/apt” listings has a significant impact on rents, such as downtown and northwest China, the government can limit the number of listings and set the limit of short-term rental days to ease the efficient response of the short-term rental market to long-term housing rents and protect the housing affordability of residents. For the downtown and southern areas where Airbnb “Entire home/apt” prices have a significant impact on rents, the government can balance housing supply and demand through tax policies, market guidance and other means to avoid further rent increases. In addition, for areas with less impact or market sensitivity, appropriate relaxation of regulatory policies to promote the flexible operation of the market, so as to achieve accurate and efficient regulation of the housing market. This kind of policy making based on local conditions not only helps to stabilize the rent level, but also alleviates the uneven development of the housing market between different regions.

### 4.Policy Recommendations to Mitigate Airbnb’s Impact on Housing Markets
Short-term rentals, particularly through platforms like Airbnb, have had a noticeable impact on housing markets. While they offer some benefits, they also contribute to higher rents and displacement of local residents. Based on recent research, this essay suggests three key policy actions to reduce the negative effects of short-term rentals.
1. Strengthen Regulation of Short-Term Rentals
Governments should introduce stricter rules on short-term rentals to limit their impact on the availability of long-term rental properties. Studies show that platforms like Airbnb reduce the supply of rental homes, leading to higher rents (Zervas et al., 2017). By limiting the number of properties a single host can rent out, requiring hosts to register, and enforcing tax rules, governments can help keep rental markets open for long-term tenants.
2. Promote Affordable Housing Development
Encouraging the building of more affordable housing is another important step. Research suggests that short-term rentals can push up the prices of housing, especially in neighborhoods where hosts often rent out properties (Gurran & Phibbs, 2017). Governments can offer incentives or subsidies to developers to build affordable homes, ensuring that lower-income groups are not pushed out of the housing market.
3. Improve Data Collection and Monitoring
Governments should improve how they collect data on short-term rental activity. Having detailed information on how often properties are rented, the prices charged, and which areas are most affected will help policymakers better understand the impact of short-term rentals. This can lead to more effective and targeted policies (Crompton, 2020).
To protect local housing markets, governments should regulate short-term rentals more effectively, support affordable housing, and improve data collection. These steps can help ensure that housing remains affordable and accessible for residents while still allowing for the benefits of short-term rentals.



:::

## Sustainable Authorship Tools

Using the Terminal in Docker, you compile the Quarto report using `quarto render <group_submission_file>.qmd`.

Your QMD file should automatically download your BibTeX and CLS files and any other required files. If this is done right after library loading then the entire report should output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References
Baker, J., Smith, L. & Clark, R. (2023) 'Regulating short-term rentals: Opportunities and challenges in urban tourism', Journal of Urban Studies, 52(3), pp. 181-198.
Crompton, S. (2020). Short-term rentals and their impact on the housing market: A study of Airbnb in the UK. Journal of Housing Studies, 32(3), 259-276.
Garay, L., Morales, E. & Wilson, D. (2020) 'The impact of short-term rentals on housing markets and local communities', Urban Affairs Review, 56(6), pp. 1248-1267.
Gurran, N., & Phibbs, P. (2017). The influence of short-term rental platforms on urban housing markets: An Australian perspective. Urban Studies, 54(1), 19-40.
Zervas, G., Proserpio, D., & Byers, J. W. (2017). The rise of the sharing economy: Estimating the impact of Airbnb on the hotel industry. Journal of Marketing Research, 54(5), 687-705.
